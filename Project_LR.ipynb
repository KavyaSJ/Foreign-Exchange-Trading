{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f83966a-2594-4736-9006-6278f6f66f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abecdab8-2a06-4e6e-94ba-5fb3a31f3220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>EMA_50</th>\n",
       "      <th>EMA_200</th>\n",
       "      <th>dist_ema_200</th>\n",
       "      <th>ema_crossover</th>\n",
       "      <th>RSI</th>\n",
       "      <th>RSI_overbought</th>\n",
       "      <th>RSI_oversold</th>\n",
       "      <th>RSI_velocity</th>\n",
       "      <th>ATR</th>\n",
       "      <th>volatility</th>\n",
       "      <th>ADX</th>\n",
       "      <th>trending</th>\n",
       "      <th>ranging</th>\n",
       "      <th>BB_lower</th>\n",
       "      <th>BB_middle</th>\n",
       "      <th>BB_upper</th>\n",
       "      <th>percent_b</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>london_session</th>\n",
       "      <th>ny_session</th>\n",
       "      <th>overlap_session</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>log_return</th>\n",
       "      <th>log_return_lag1</th>\n",
       "      <th>RSI_lag1</th>\n",
       "      <th>log_return_lag3</th>\n",
       "      <th>RSI_lag3</th>\n",
       "      <th>log_return_lag5</th>\n",
       "      <th>RSI_lag5</th>\n",
       "      <th>fractal_dim</th>\n",
       "      <th>EMA_200_1H</th>\n",
       "      <th>RSI_1H</th>\n",
       "      <th>ATR_1H</th>\n",
       "      <th>ADX_1H</th>\n",
       "      <th>EMA_200_4H</th>\n",
       "      <th>RSI_4H</th>\n",
       "      <th>ATR_4H</th>\n",
       "      <th>ADX_4H</th>\n",
       "      <th>EMA_200_D</th>\n",
       "      <th>RSI_D</th>\n",
       "      <th>ATR_D</th>\n",
       "      <th>ADX_D</th>\n",
       "      <th>price_vs_EMA200_1H</th>\n",
       "      <th>price_vs_EMA200_4H</th>\n",
       "      <th>price_vs_EMA200_D</th>\n",
       "      <th>RSI_dist_50_1H</th>\n",
       "      <th>RSI_dist_50_4H</th>\n",
       "      <th>RSI_dist_50_D</th>\n",
       "      <th>ATR_ratio_1H_4H</th>\n",
       "      <th>ATR_ratio_4H_D</th>\n",
       "      <th>trend_1H</th>\n",
       "      <th>trend_4H</th>\n",
       "      <th>trend_D</th>\n",
       "      <th>range_1H</th>\n",
       "      <th>range_4H</th>\n",
       "      <th>bull_1H</th>\n",
       "      <th>bull_4H</th>\n",
       "      <th>bull_D</th>\n",
       "      <th>trend_alignment_score</th>\n",
       "      <th>wti_close</th>\n",
       "      <th>wti_logret_15m</th>\n",
       "      <th>wti_mom_1h</th>\n",
       "      <th>wti_mom_4h</th>\n",
       "      <th>udx_close</th>\n",
       "      <th>udx_logret_15m</th>\n",
       "      <th>udx_mom_1h</th>\n",
       "      <th>udx_mom_4h</th>\n",
       "      <th>us10y</th>\n",
       "      <th>ca10y</th>\n",
       "      <th>us_ca_spread</th>\n",
       "      <th>ys_change</th>\n",
       "      <th>ys_z60</th>\n",
       "      <th>yield_available</th>\n",
       "      <th>wti_available</th>\n",
       "      <th>udx_available</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-07-30 03:45:00+00:00</td>\n",
       "      <td>1.00399</td>\n",
       "      <td>1.00400</td>\n",
       "      <td>1.00366</td>\n",
       "      <td>1.00368</td>\n",
       "      <td>707.39</td>\n",
       "      <td>1.003828</td>\n",
       "      <td>1.004002</td>\n",
       "      <td>-0.000320</td>\n",
       "      <td>0</td>\n",
       "      <td>40.430022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-11.849016</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>36.089846</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.003635</td>\n",
       "      <td>1.004399</td>\n",
       "      <td>1.005162</td>\n",
       "      <td>0.029514</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>45.930948</td>\n",
       "      <td>-0.000418</td>\n",
       "      <td>52.279038</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>57.276024</td>\n",
       "      <td>1.428451</td>\n",
       "      <td>1.009113</td>\n",
       "      <td>82.299723</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>65.085442</td>\n",
       "      <td>1.014954</td>\n",
       "      <td>37.117655</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>58.043153</td>\n",
       "      <td>1.006425</td>\n",
       "      <td>38.079194</td>\n",
       "      <td>0.004891</td>\n",
       "      <td>14.44241</td>\n",
       "      <td>-15.590156</td>\n",
       "      <td>-9.404533</td>\n",
       "      <td>-0.561344</td>\n",
       "      <td>32.299723</td>\n",
       "      <td>-12.882345</td>\n",
       "      <td>-11.920806</td>\n",
       "      <td>0.290668</td>\n",
       "      <td>0.245129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90.79</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.007185</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>13010.0</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-1.380939</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-07-30 04:00:00+00:00</td>\n",
       "      <td>1.00367</td>\n",
       "      <td>1.00384</td>\n",
       "      <td>1.00366</td>\n",
       "      <td>1.00375</td>\n",
       "      <td>359.86</td>\n",
       "      <td>1.003825</td>\n",
       "      <td>1.003999</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>0</td>\n",
       "      <td>42.170394</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.068694</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>34.492336</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.003628</td>\n",
       "      <td>1.004397</td>\n",
       "      <td>1.005166</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>40.430022</td>\n",
       "      <td>-0.000339</td>\n",
       "      <td>45.239088</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>63.638614</td>\n",
       "      <td>1.430065</td>\n",
       "      <td>1.009059</td>\n",
       "      <td>53.322815</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>62.486659</td>\n",
       "      <td>1.014842</td>\n",
       "      <td>34.713733</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>55.722413</td>\n",
       "      <td>1.006425</td>\n",
       "      <td>38.079194</td>\n",
       "      <td>0.004891</td>\n",
       "      <td>14.44241</td>\n",
       "      <td>-13.153844</td>\n",
       "      <td>-9.126858</td>\n",
       "      <td>-0.547031</td>\n",
       "      <td>3.322815</td>\n",
       "      <td>-15.286267</td>\n",
       "      <td>-11.920806</td>\n",
       "      <td>0.332064</td>\n",
       "      <td>0.248505</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90.73</td>\n",
       "      <td>-0.000661</td>\n",
       "      <td>0.006302</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>13009.0</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-1.380939</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-07-30 04:15:00+00:00</td>\n",
       "      <td>1.00377</td>\n",
       "      <td>1.00428</td>\n",
       "      <td>1.00376</td>\n",
       "      <td>1.00415</td>\n",
       "      <td>405.63</td>\n",
       "      <td>1.003837</td>\n",
       "      <td>1.004001</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0</td>\n",
       "      <td>50.983062</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.052115</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>32.643937</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.003679</td>\n",
       "      <td>1.004412</td>\n",
       "      <td>1.005146</td>\n",
       "      <td>0.320985</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>42.170394</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>45.930948</td>\n",
       "      <td>-0.000418</td>\n",
       "      <td>52.279038</td>\n",
       "      <td>1.429619</td>\n",
       "      <td>1.009059</td>\n",
       "      <td>53.322815</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>62.486659</td>\n",
       "      <td>1.014842</td>\n",
       "      <td>34.713733</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>55.722413</td>\n",
       "      <td>1.006425</td>\n",
       "      <td>38.079194</td>\n",
       "      <td>0.004891</td>\n",
       "      <td>14.44241</td>\n",
       "      <td>-12.162695</td>\n",
       "      <td>-8.797734</td>\n",
       "      <td>-0.465242</td>\n",
       "      <td>3.322815</td>\n",
       "      <td>-15.286267</td>\n",
       "      <td>-11.920806</td>\n",
       "      <td>0.332064</td>\n",
       "      <td>0.248505</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90.64</td>\n",
       "      <td>-0.000992</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.007086</td>\n",
       "      <td>13008.0</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000615</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-1.380939</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-07-30 04:30:00+00:00</td>\n",
       "      <td>1.00415</td>\n",
       "      <td>1.00462</td>\n",
       "      <td>1.00415</td>\n",
       "      <td>1.00448</td>\n",
       "      <td>960.78</td>\n",
       "      <td>1.003863</td>\n",
       "      <td>1.004005</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0</td>\n",
       "      <td>56.828214</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.398192</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>31.844678</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.003713</td>\n",
       "      <td>1.004431</td>\n",
       "      <td>1.005150</td>\n",
       "      <td>0.533745</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>50.983062</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>40.430022</td>\n",
       "      <td>-0.000339</td>\n",
       "      <td>45.239088</td>\n",
       "      <td>1.412860</td>\n",
       "      <td>1.009059</td>\n",
       "      <td>53.322815</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>62.486659</td>\n",
       "      <td>1.014842</td>\n",
       "      <td>34.713733</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>55.722413</td>\n",
       "      <td>1.006425</td>\n",
       "      <td>38.079194</td>\n",
       "      <td>0.004891</td>\n",
       "      <td>14.44241</td>\n",
       "      <td>-11.344996</td>\n",
       "      <td>-8.526206</td>\n",
       "      <td>-0.397766</td>\n",
       "      <td>3.322815</td>\n",
       "      <td>-15.286267</td>\n",
       "      <td>-11.920806</td>\n",
       "      <td>0.332064</td>\n",
       "      <td>0.248505</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90.49</td>\n",
       "      <td>-0.001656</td>\n",
       "      <td>-0.002649</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>12996.0</td>\n",
       "      <td>-0.000923</td>\n",
       "      <td>-0.001384</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-1.380939</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-07-30 04:45:00+00:00</td>\n",
       "      <td>1.00451</td>\n",
       "      <td>1.00469</td>\n",
       "      <td>1.00441</td>\n",
       "      <td>1.00446</td>\n",
       "      <td>692.64</td>\n",
       "      <td>1.003886</td>\n",
       "      <td>1.004010</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0</td>\n",
       "      <td>56.389333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.218939</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>31.272003</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.003723</td>\n",
       "      <td>1.004440</td>\n",
       "      <td>1.005156</td>\n",
       "      <td>0.514315</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>56.828214</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>42.170394</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>45.930948</td>\n",
       "      <td>1.405050</td>\n",
       "      <td>1.009059</td>\n",
       "      <td>53.322815</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>62.486659</td>\n",
       "      <td>1.014842</td>\n",
       "      <td>34.713733</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>55.722413</td>\n",
       "      <td>1.006425</td>\n",
       "      <td>38.079194</td>\n",
       "      <td>0.004891</td>\n",
       "      <td>14.44241</td>\n",
       "      <td>-11.394554</td>\n",
       "      <td>-8.542662</td>\n",
       "      <td>-0.401855</td>\n",
       "      <td>3.322815</td>\n",
       "      <td>-15.286267</td>\n",
       "      <td>-11.920806</td>\n",
       "      <td>0.332064</td>\n",
       "      <td>0.248505</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90.61</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>-0.001985</td>\n",
       "      <td>0.006977</td>\n",
       "      <td>12995.0</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.001154</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-1.380939</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp     open     high      low    close  volume  \\\n",
       "0  2012-07-30 03:45:00+00:00  1.00399  1.00400  1.00366  1.00368  707.39   \n",
       "1  2012-07-30 04:00:00+00:00  1.00367  1.00384  1.00366  1.00375  359.86   \n",
       "2  2012-07-30 04:15:00+00:00  1.00377  1.00428  1.00376  1.00415  405.63   \n",
       "3  2012-07-30 04:30:00+00:00  1.00415  1.00462  1.00415  1.00448  960.78   \n",
       "4  2012-07-30 04:45:00+00:00  1.00451  1.00469  1.00441  1.00446  692.64   \n",
       "\n",
       "     EMA_50   EMA_200  dist_ema_200  ema_crossover        RSI  RSI_overbought  \\\n",
       "0  1.003828  1.004002     -0.000320              0  40.430022               0   \n",
       "1  1.003825  1.003999     -0.000248              0  42.170394               0   \n",
       "2  1.003837  1.004001      0.000149              0  50.983062               0   \n",
       "3  1.003863  1.004005      0.000473              0  56.828214               0   \n",
       "4  1.003886  1.004010      0.000448              0  56.389333               0   \n",
       "\n",
       "   RSI_oversold  RSI_velocity       ATR  volatility        ADX  trending  \\\n",
       "0             0    -11.849016  0.000405    0.000403  36.089846         1   \n",
       "1             0     -3.068694  0.000389    0.000387  34.492336         1   \n",
       "2             0      5.052115  0.000399    0.000397  32.643937         1   \n",
       "3             0     16.398192  0.000404    0.000402  31.844678         1   \n",
       "4             0     14.218939  0.000395    0.000393  31.272003         1   \n",
       "\n",
       "   ranging  BB_lower  BB_middle  BB_upper  percent_b  hour  day_of_week  \\\n",
       "0        0  1.003635   1.004399  1.005162   0.029514     3            0   \n",
       "1        0  1.003628   1.004397  1.005166   0.079208     4            0   \n",
       "2        0  1.003679   1.004412  1.005146   0.320985     4            0   \n",
       "3        0  1.003713   1.004431  1.005150   0.533745     4            0   \n",
       "4        0  1.003723   1.004440  1.005156   0.514315     4            0   \n",
       "\n",
       "   london_session  ny_session  overlap_session  hour_sin  hour_cos  \\\n",
       "0               0           0                0  0.707107  0.707107   \n",
       "1               0           0                0  0.866025  0.500000   \n",
       "2               0           0                0  0.866025  0.500000   \n",
       "3               0           0                0  0.866025  0.500000   \n",
       "4               0           0                0  0.866025  0.500000   \n",
       "\n",
       "   log_return  log_return_lag1   RSI_lag1  log_return_lag3   RSI_lag3  \\\n",
       "0   -0.000299         0.000030  45.930948        -0.000418  52.279038   \n",
       "1    0.000070        -0.000299  40.430022        -0.000339  45.239088   \n",
       "2    0.000398         0.000070  42.170394         0.000030  45.930948   \n",
       "3    0.000329         0.000398  50.983062        -0.000299  40.430022   \n",
       "4   -0.000020         0.000329  56.828214         0.000070  42.170394   \n",
       "\n",
       "   log_return_lag5   RSI_lag5  fractal_dim  EMA_200_1H     RSI_1H    ATR_1H  \\\n",
       "0        -0.000299  57.276024     1.428451    1.009113  82.299723  0.000348   \n",
       "1         0.000309  63.638614     1.430065    1.009059  53.322815  0.000404   \n",
       "2        -0.000418  52.279038     1.429619    1.009059  53.322815  0.000404   \n",
       "3        -0.000339  45.239088     1.412860    1.009059  53.322815  0.000404   \n",
       "4         0.000030  45.930948     1.405050    1.009059  53.322815  0.000404   \n",
       "\n",
       "      ADX_1H  EMA_200_4H     RSI_4H    ATR_4H     ADX_4H  EMA_200_D  \\\n",
       "0  65.085442    1.014954  37.117655  0.001199  58.043153   1.006425   \n",
       "1  62.486659    1.014842  34.713733  0.001215  55.722413   1.006425   \n",
       "2  62.486659    1.014842  34.713733  0.001215  55.722413   1.006425   \n",
       "3  62.486659    1.014842  34.713733  0.001215  55.722413   1.006425   \n",
       "4  62.486659    1.014842  34.713733  0.001215  55.722413   1.006425   \n",
       "\n",
       "       RSI_D     ATR_D     ADX_D  price_vs_EMA200_1H  price_vs_EMA200_4H  \\\n",
       "0  38.079194  0.004891  14.44241          -15.590156           -9.404533   \n",
       "1  38.079194  0.004891  14.44241          -13.153844           -9.126858   \n",
       "2  38.079194  0.004891  14.44241          -12.162695           -8.797734   \n",
       "3  38.079194  0.004891  14.44241          -11.344996           -8.526206   \n",
       "4  38.079194  0.004891  14.44241          -11.394554           -8.542662   \n",
       "\n",
       "   price_vs_EMA200_D  RSI_dist_50_1H  RSI_dist_50_4H  RSI_dist_50_D  \\\n",
       "0          -0.561344       32.299723      -12.882345     -11.920806   \n",
       "1          -0.547031        3.322815      -15.286267     -11.920806   \n",
       "2          -0.465242        3.322815      -15.286267     -11.920806   \n",
       "3          -0.397766        3.322815      -15.286267     -11.920806   \n",
       "4          -0.401855        3.322815      -15.286267     -11.920806   \n",
       "\n",
       "   ATR_ratio_1H_4H  ATR_ratio_4H_D  trend_1H  trend_4H  trend_D  range_1H  \\\n",
       "0         0.290668        0.245129         1         1        0         0   \n",
       "1         0.332064        0.248505         1         1        0         0   \n",
       "2         0.332064        0.248505         1         1        0         0   \n",
       "3         0.332064        0.248505         1         1        0         0   \n",
       "4         0.332064        0.248505         1         1        0         0   \n",
       "\n",
       "   range_4H  bull_1H  bull_4H  bull_D  trend_alignment_score  wti_close  \\\n",
       "0         0        0        0       0                      0      90.79   \n",
       "1         0        0        0       0                      0      90.73   \n",
       "2         0        0        0       0                      0      90.64   \n",
       "3         0        0        0       0                      0      90.49   \n",
       "4         0        0        0       0                      0      90.61   \n",
       "\n",
       "   wti_logret_15m  wti_mom_1h  wti_mom_4h  udx_close  udx_logret_15m  \\\n",
       "0        0.000661    0.007185    0.009740    13010.0       -0.000307   \n",
       "1       -0.000661    0.006302    0.009079    13009.0       -0.000077   \n",
       "2       -0.000992    0.000662    0.007086    13008.0       -0.000077   \n",
       "3       -0.001656   -0.002649    0.004763    12996.0       -0.000923   \n",
       "4        0.001325   -0.001985    0.006977    12995.0       -0.000077   \n",
       "\n",
       "   udx_mom_1h  udx_mom_4h  us10y  ca10y  us_ca_spread  ys_change    ys_z60  \\\n",
       "0    0.000384    0.001000   1.58   1.75         -0.17       0.02 -1.380939   \n",
       "1    0.000000    0.000923   1.58   1.75         -0.17       0.02 -1.380939   \n",
       "2   -0.000615    0.001077   1.58   1.75         -0.17       0.02 -1.380939   \n",
       "3   -0.001384    0.000077   1.58   1.75         -0.17       0.02 -1.380939   \n",
       "4   -0.001154    0.000154   1.58   1.75         -0.17       0.02 -1.380939   \n",
       "\n",
       "   yield_available  wti_available  udx_available  target  \n",
       "0              1.0              1              1       1  \n",
       "1              1.0              1              1       1  \n",
       "2              1.0              1              1       1  \n",
       "3              1.0              1              1       1  \n",
       "4              1.0              1              1       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"E:\\\\Term Project_ML\\\\final_dataset.csv\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e3c34c-c02e-4af5-8e88-468675ab372c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'open', 'high', 'low', 'close', 'volume', 'EMA_50',\n",
       "       'EMA_200', 'dist_ema_200', 'ema_crossover', 'RSI', 'RSI_overbought',\n",
       "       'RSI_oversold', 'RSI_velocity', 'ATR', 'volatility', 'ADX', 'trending',\n",
       "       'ranging', 'BB_lower', 'BB_middle', 'BB_upper', 'percent_b', 'hour',\n",
       "       'day_of_week', 'london_session', 'ny_session', 'overlap_session',\n",
       "       'hour_sin', 'hour_cos', 'log_return', 'log_return_lag1', 'RSI_lag1',\n",
       "       'log_return_lag3', 'RSI_lag3', 'log_return_lag5', 'RSI_lag5',\n",
       "       'fractal_dim', 'EMA_200_1H', 'RSI_1H', 'ATR_1H', 'ADX_1H', 'EMA_200_4H',\n",
       "       'RSI_4H', 'ATR_4H', 'ADX_4H', 'EMA_200_D', 'RSI_D', 'ATR_D', 'ADX_D',\n",
       "       'price_vs_EMA200_1H', 'price_vs_EMA200_4H', 'price_vs_EMA200_D',\n",
       "       'RSI_dist_50_1H', 'RSI_dist_50_4H', 'RSI_dist_50_D', 'ATR_ratio_1H_4H',\n",
       "       'ATR_ratio_4H_D', 'trend_1H', 'trend_4H', 'trend_D', 'range_1H',\n",
       "       'range_4H', 'bull_1H', 'bull_4H', 'bull_D', 'trend_alignment_score',\n",
       "       'wti_close', 'wti_logret_15m', 'wti_mom_1h', 'wti_mom_4h', 'udx_close',\n",
       "       'udx_logret_15m', 'udx_mom_1h', 'udx_mom_4h', 'us10y', 'ca10y',\n",
       "       'us_ca_spread', 'ys_change', 'ys_z60', 'yield_available',\n",
       "       'wti_available', 'udx_available', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b390a2b4-4212-4f52-9c61-522392f2e7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].unique()  #target = 1 → Buy, target = -1 → Sell,target = 0 → No trade / Hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd14f19-22e8-497a-810f-92352c2de689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "X = df.drop(columns=['target', 'timestamp'])\n",
    "y = df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c29321ce-a3a2-451c-8106-0cccd9493450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinities\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill missing values using training statistics later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df3557ee-69b1-4e7d-8242-b88a397d11df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84d8b4dc-903f-4809-b142-aa64002354c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        max_iter=200,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea02bba6-fdb0-4d2b-b107-e0c4fd5774f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Fill NaNs using TRAIN medians only (no leakage)\n",
    "    medians = X_train.median()\n",
    "    X_train = X_train.fillna(medians)\n",
    "    X_test  = X_test.fillna(medians)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    all_preds.extend(preds)\n",
    "    all_true.extend(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e0a4629-60cb-4c8c-a487-5780b50b3b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.42      0.41      0.42     97752\n",
      "           0       0.51      0.74      0.60     78332\n",
      "           1       0.47      0.32      0.38    100036\n",
      "\n",
      "    accuracy                           0.47    276120\n",
      "   macro avg       0.47      0.49      0.47    276120\n",
      "weighted avg       0.46      0.47      0.45    276120\n",
      "\n",
      "[[40068 28514 29170]\n",
      " [13591 57982  6759]\n",
      " [40736 27787 31513]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_true, all_preds))\n",
    "print(confusion_matrix(all_true, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5954e166-1c1b-49af-99f0-bb78663fe700",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_mask = y != 0\n",
    "\n",
    "X_trade = X[trade_mask]\n",
    "y_trade = y[trade_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17325abf-f861-4134-bd38-e4fae8e061ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "volatility          -0.499202\n",
       "ATR                  0.345151\n",
       "hour_cos            -0.310321\n",
       "EMA_200_D            0.304193\n",
       "hour_sin             0.284306\n",
       "ATR_4H               0.133923\n",
       "price_vs_EMA200_D    0.125481\n",
       "EMA_200_4H           0.110554\n",
       "low                 -0.104387\n",
       "udx_mom_4h           0.094615\n",
       "ATR_D               -0.090474\n",
       "ATR_1H               0.089820\n",
       "ATR_ratio_4H_D      -0.087391\n",
       "ny_session          -0.073003\n",
       "open                -0.063404\n",
       "ATR_ratio_1H_4H     -0.063400\n",
       "close               -0.061927\n",
       "EMA_200             -0.056056\n",
       "EMA_200_1H          -0.049763\n",
       "us_ca_spread        -0.045681\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "feature_importance = pd.Series(\n",
    "    model.named_steps['clf'].coef_[0],\n",
    "    index=X.columns\n",
    ").sort_values(key=abs, ascending=False)\n",
    "\n",
    "feature_importance.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60a4350c-df83-4e53-91a7-04c7f870d41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mozahid\\AppData\\Local\\Temp\\ipykernel_20344\\171837779.py:8: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=top_features.values, y=top_features.index, palette=\"viridis\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use the same feature_importance Series you already have\n",
    "top_features = feature_importance.head(20)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=top_features.values, y=top_features.index, palette=\"viridis\")\n",
    "plt.title(\"Top 20 Feature Importances (Logistic Regression Coefficients)\")\n",
    "plt.xlabel(\"Coefficient value (positive → Buy, negative → Sell)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3221a97-6433-4052-a000-fd0c90d0128f",
   "metadata": {},
   "source": [
    "### NEXT STAGE: Improve the Baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a5eba4-d730-4191-a937-903f28483ee7",
   "metadata": {},
   "source": [
    "Tune C (regularization strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bf3f819-e4de-453e-9d80-754c1085faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93be406d-fbbe-4e02-9e17-a631b0594647",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('clf', LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        max_iter=500,\n",
    "        class_weight='balanced',\n",
    "        solver='lbfgs'\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cc809eb-165b-41d5-9b05-bab2f14210dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 0.01, 'clf__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'clf__C': [0.01, 0.1, 1, 5, 10],\n",
    "    'clf__penalty': ['l2']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=tscv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gs.fit(X, y)\n",
    "print(gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a7455b8-5186-4c28-95bc-49f42fed193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('clf', LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        max_iter=500,\n",
    "        class_weight='balanced',\n",
    "        C=0.01,\n",
    "        penalty='l2',\n",
    "        solver='lbfgs'\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69b8c02c-a6a5-4758-bfee-3ceca3e6c909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.43      0.41      0.42     97752\n",
      "           0       0.52      0.77      0.62     78332\n",
      "           1       0.47      0.31      0.37    100036\n",
      "\n",
      "    accuracy                           0.48    276120\n",
      "   macro avg       0.47      0.50      0.47    276120\n",
      "weighted avg       0.47      0.48      0.46    276120\n",
      "\n",
      "[[40500 28469 28783]\n",
      " [11922 60106  6304]\n",
      " [41153 27793 31090]]\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "    preds = best_model.predict(X_test)\n",
    "\n",
    "    all_preds.extend(preds)\n",
    "    all_true.extend(y_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(all_true, all_preds))\n",
    "print(confusion_matrix(all_true, all_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9004eaa0-b07a-441d-a966-9ee1c768efbc",
   "metadata": {},
   "source": [
    "#### class_weight = {-1:1.5, 0:0.5, 1:1.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1169e7e6-ba61-498a-a165-f9a762e5d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.41      0.44      0.42     97752\n",
      "           0       0.56      0.62      0.59     78332\n",
      "           1       0.45      0.38      0.41    100036\n",
      "\n",
      "    accuracy                           0.47    276120\n",
      "   macro avg       0.47      0.48      0.47    276120\n",
      "weighted avg       0.47      0.47      0.47    276120\n",
      "\n",
      "[[42546 19929 35277]\n",
      " [18230 48687 11415]\n",
      " [43142 18656 38238]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('clf', LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        penalty='l2',       # stable\n",
    "        solver='lbfgs',\n",
    "        C=0.1,              # mild regularization\n",
    "        class_weight={-1:1.2, 0:1.0, 1:1.2},\n",
    "        max_iter=500,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X_reduced):\n",
    "    X_train, X_test = X_reduced.iloc[train_idx], X_reduced.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    all_preds.extend(preds)\n",
    "    all_true.extend(y_test)\n",
    "\n",
    "print(classification_report(all_true, all_preds))\n",
    "print(confusion_matrix(all_true, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73edc873-b3a7-4b4a-81a9-bdc87c6f18f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping highly correlated features: ['high', 'low', 'close', 'EMA_50', 'EMA_200', 'volatility', 'BB_lower', 'BB_middle', 'BB_upper', 'RSI_lag1', 'EMA_200_1H', 'EMA_200_4H', 'EMA_200_D', 'ATR_D', 'price_vs_EMA200_4H', 'RSI_dist_50_1H', 'RSI_dist_50_4H', 'RSI_dist_50_D', 'ca10y']\n",
      "Number of features after manual selection: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.41      0.18      0.25     97752\n",
      "           0       0.53      0.68      0.60     78332\n",
      "           1       0.45      0.59      0.51    100036\n",
      "\n",
      "    accuracy                           0.47    276120\n",
      "   macro avg       0.46      0.49      0.45    276120\n",
      "weighted avg       0.46      0.47      0.44    276120\n",
      "\n",
      "[[17942 23867 55943]\n",
      " [ 7423 53657 17252]\n",
      " [17871 23336 58829]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# 1️ Prepare features & target\n",
    "# -----------------------------\n",
    "X = df.drop(columns=['target', 'timestamp'])\n",
    "y = df['target']\n",
    "\n",
    "# Replace infinities\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2️ Drop highly correlated features\n",
    "# -----------------------------\n",
    "corr_matrix = X.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.9)]\n",
    "print(\"Dropping highly correlated features:\", to_drop)\n",
    "X = X.drop(columns=to_drop)\n",
    "\n",
    "# -----------------------------\n",
    "# 3️ Manual selection of strongest indicators\n",
    "# -----------------------------\n",
    "# Keep one per type/timeframe (example)\n",
    "manual_keep = [\n",
    "    'EMA_50', 'EMA_200', 'RSI', 'ATR', 'ADX',\n",
    "    'EMA_200_1H', 'RSI_1H', 'ATR_1H', 'ADX_1H',\n",
    "    'EMA_200_4H', 'RSI_4H', 'ATR_4H', 'ADX_4H',\n",
    "    'EMA_200_D', 'RSI_D', 'ATR_D', 'ADX_D',\n",
    "    'log_return', 'log_return_lag1', 'log_return_lag3', 'log_return_lag5',\n",
    "    'hour_sin', 'hour_cos', 'day_of_week'\n",
    "]\n",
    "\n",
    "X = X[[col for col in X.columns if col in manual_keep]]\n",
    "print(\"Number of features after manual selection:\", X.shape[1])\n",
    "\n",
    "# -----------------------------\n",
    "# 4️ Define TimeSeriesSplit\n",
    "# -----------------------------\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# -----------------------------\n",
    "# 5️ Define Logistic Regression pipeline\n",
    "# -----------------------------\n",
    "model = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('clf', LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        penalty='l2',               # stable for multiple correlated features\n",
    "        solver='lbfgs',\n",
    "        C=0.1,                      # mild regularization\n",
    "        class_weight={-1:1.2, 0:1.0, 1:1.2},  # boost ±1 signals\n",
    "        max_iter=500,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 6️ Run TimeSeries CV\n",
    "# -----------------------------\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    all_preds.extend(preds)\n",
    "    all_true.extend(y_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 7️ Evaluate\n",
    "# -----------------------------\n",
    "print(classification_report(all_true, all_preds))\n",
    "print(confusion_matrix(all_true, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcfd01b3-f66a-4cb6-bc56-08f1179e3427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "E:\\Users\\Mozahid\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.51      0.45     97752\n",
      "           0       0.59      0.51      0.55     78332\n",
      "           1       0.43      0.36      0.39    100036\n",
      "\n",
      "    accuracy                           0.46    276120\n",
      "   macro avg       0.47      0.46      0.46    276120\n",
      "weighted avg       0.47      0.46      0.46    276120\n",
      "\n",
      "[[50091 13730 33931]\n",
      " [24454 39963 13915]\n",
      " [50422 13670 35944]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Prepare features & target\n",
    "# -----------------------------\n",
    "X = df.drop(columns=['target', 'timestamp'])\n",
    "y = df['target']\n",
    "\n",
    "# Replace infinities\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Drop highly correlated features\n",
    "# -----------------------------\n",
    "corr_matrix = X.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.9)]\n",
    "X = X.drop(columns=to_drop)\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Manual selection of strong indicators\n",
    "# -----------------------------\n",
    "manual_keep = [\n",
    "    'EMA_50', 'EMA_200', 'RSI', 'ATR', 'ADX',\n",
    "    'EMA_200_1H', 'RSI_1H', 'ATR_1H', 'ADX_1H',\n",
    "    'EMA_200_4H', 'RSI_4H', 'ATR_4H', 'ADX_4H',\n",
    "    'EMA_200_D', 'RSI_D', 'ATR_D', 'ADX_D',\n",
    "    'log_return', 'log_return_lag1', 'log_return_lag3', 'log_return_lag5',\n",
    "    'hour_sin', 'hour_cos', 'day_of_week'\n",
    "]\n",
    "X = X[[col for col in X.columns if col in manual_keep]]\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ TimeSeries split\n",
    "# -----------------------------\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Stage 1: Trade vs Hold\n",
    "# -----------------------------\n",
    "stage1_model = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('clf', LogisticRegression(\n",
    "        multi_class='auto',\n",
    "        solver='lbfgs',\n",
    "        C=0.1,\n",
    "        class_weight={-1:1.1, 0:1.0, 1:1.1},\n",
    "        max_iter=500,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Stage 2: BUY vs SELL\n",
    "# -----------------------------\n",
    "stage2_model = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('clf', LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        solver='lbfgs',\n",
    "        C=0.1,\n",
    "        class_weight={-1:1.3, 1:1.2},  # slightly boost SELL\n",
    "        max_iter=500,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 7️⃣ Run TimeSeries CV\n",
    "# -----------------------------\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Stage 1: Trade vs Hold\n",
    "    y_train_stage1 = y_train.copy()\n",
    "    y_train_stage1[y_train_stage1 != 0] = 1  # Trades = 1, HOLD = 0\n",
    "    stage1_model.fit(X_train, y_train_stage1)\n",
    "    stage1_pred = stage1_model.predict(X_test)\n",
    "\n",
    "    # Stage 2: BUY vs SELL on predicted trades\n",
    "    preds = []\n",
    "    trade_indices = np.where(stage1_pred == 1)[0]\n",
    "    hold_indices = np.where(stage1_pred == 0)[0]\n",
    "\n",
    "    if len(trade_indices) > 0:\n",
    "        X_test_trades = X_test.iloc[trade_indices]\n",
    "        y_train_trades = y_train[y_train != 0]  # only train on trades\n",
    "        stage2_model.fit(X_train[y_train != 0], y_train_trades)\n",
    "        stage2_pred = stage2_model.predict(X_test_trades)\n",
    "\n",
    "    # Combine predictions\n",
    "    temp_preds = np.zeros(len(X_test), dtype=int)\n",
    "    temp_preds[hold_indices] = 0  # HOLD\n",
    "    if len(trade_indices) > 0:\n",
    "        temp_preds[trade_indices] = stage2_pred\n",
    "\n",
    "    all_preds.extend(temp_preds)\n",
    "    all_true.extend(y_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 8️⃣ Evaluate\n",
    "# -----------------------------\n",
    "print(classification_report(all_true, all_preds))\n",
    "print(confusion_matrix(all_true, all_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba3f77b-37a9-493a-b4c5-95c51b9e19d5",
   "metadata": {},
   "source": [
    "### Two-stage pipeline using LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddc007e0-580c-4517-9f33-8b21f5e0d9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 39716, number of negative: 15508\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3869\n",
      "[LightGBM] [Info] Number of data points in the train set: 55224, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.754493 -> initscore=1.122720\n",
      "[LightGBM] [Info] Start training from score 1.122720\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3866\n",
      "[LightGBM] [Info] Number of data points in the train set: 39716, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.684796\n",
      "[LightGBM] [Info] Start training from score -0.701569\n",
      "[LightGBM] [Info] Number of positive: 78300, number of negative: 32148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3872\n",
      "[LightGBM] [Info] Number of data points in the train set: 110448, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.745076 -> initscore=1.072519\n",
      "[LightGBM] [Info] Start training from score 1.072519\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3872\n",
      "[LightGBM] [Info] Number of data points in the train set: 78300, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.681807\n",
      "[LightGBM] [Info] Start training from score -0.704618\n",
      "[LightGBM] [Info] Number of positive: 117263, number of negative: 48409\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3872\n",
      "[LightGBM] [Info] Number of data points in the train set: 165672, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.744036 -> initscore=1.067055\n",
      "[LightGBM] [Info] Start training from score 1.067055\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3872\n",
      "[LightGBM] [Info] Number of data points in the train set: 117263, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.675386\n",
      "[LightGBM] [Info] Start training from score -0.711230\n",
      "[LightGBM] [Info] Number of positive: 157356, number of negative: 63540\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3872\n",
      "[LightGBM] [Info] Number of data points in the train set: 220896, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.748224 -> initscore=1.089163\n",
      "[LightGBM] [Info] Start training from score 1.089163\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001533 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3872\n",
      "[LightGBM] [Info] Number of data points in the train set: 157356, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.671218\n",
      "[LightGBM] [Info] Start training from score -0.715568\n",
      "[LightGBM] [Info] Number of positive: 197376, number of negative: 78744\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3872\n",
      "[LightGBM] [Info] Number of data points in the train set: 276120, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.750491 -> initscore=1.101230\n",
      "[LightGBM] [Info] Start training from score 1.101230\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3872\n",
      "[LightGBM] [Info] Number of data points in the train set: 197376, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.670930\n",
      "[LightGBM] [Info] Start training from score -0.715869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.45      0.42     97752\n",
      "           0       0.58      0.50      0.54     78332\n",
      "           1       0.42      0.43      0.42    100036\n",
      "\n",
      "    accuracy                           0.46    276120\n",
      "   macro avg       0.47      0.46      0.46    276120\n",
      "weighted avg       0.46      0.46      0.46    276120\n",
      "\n",
      "[[43577 14293 39882]\n",
      " [20626 39451 18255]\n",
      " [43620 13769 42647]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Prepare features & target\n",
    "# -----------------------------\n",
    "X = df.drop(columns=['target', 'timestamp'])\n",
    "y = df['target']\n",
    "\n",
    "# Replace infinities\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Drop highly correlated features\n",
    "# -----------------------------\n",
    "corr_matrix = X.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.9)]\n",
    "X = X.drop(columns=to_drop)\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Manual selection of strong indicators\n",
    "# -----------------------------\n",
    "manual_keep = [\n",
    "    'EMA_50', 'EMA_200', 'RSI', 'ATR', 'ADX',\n",
    "    'EMA_200_1H', 'RSI_1H', 'ATR_1H', 'ADX_1H',\n",
    "    'EMA_200_4H', 'RSI_4H', 'ATR_4H', 'ADX_4H',\n",
    "    'EMA_200_D', 'RSI_D', 'ATR_D', 'ADX_D',\n",
    "    'log_return', 'log_return_lag1', 'log_return_lag3', 'log_return_lag5',\n",
    "    'hour_sin', 'hour_cos', 'day_of_week'\n",
    "]\n",
    "X = X[[col for col in X.columns if col in manual_keep]]\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ TimeSeries split\n",
    "# -----------------------------\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Stage 1: Trade vs Hold (LightGBM)\n",
    "# -----------------------------\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Fill NaNs with median\n",
    "    medians = X_train.median()\n",
    "    X_train = X_train.fillna(medians)\n",
    "    X_test = X_test.fillna(medians)\n",
    "\n",
    "    # Stage 1: Trade (±1) vs Hold (0)\n",
    "    y_train_stage1 = y_train.copy()\n",
    "    y_train_stage1[y_train_stage1 != 0] = 1  # Trades=1, HOLD=0\n",
    "\n",
    "    lgb_stage1 = lgb.LGBMClassifier(\n",
    "        objective='binary',\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.1,\n",
    "        class_weight={0:1.0, 1:1.2},\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    lgb_stage1.fit(X_train, y_train_stage1)\n",
    "    stage1_pred = lgb_stage1.predict(X_test)\n",
    "\n",
    "    # Stage 2: BUY vs SELL on predicted trades\n",
    "    trade_indices = np.where(stage1_pred == 1)[0]\n",
    "    hold_indices = np.where(stage1_pred == 0)[0]\n",
    "\n",
    "    temp_preds = np.zeros(len(X_test), dtype=int)\n",
    "    temp_preds[hold_indices] = 0  # HOLD\n",
    "\n",
    "    if len(trade_indices) > 0:\n",
    "        X_test_trades = X_test.iloc[trade_indices]\n",
    "        X_train_trades = X_train[y_train != 0]\n",
    "        y_train_trades = y_train[y_train != 0]\n",
    "\n",
    "        lgb_stage2 = lgb.LGBMClassifier(\n",
    "            objective='multiclass',\n",
    "            num_class=2,\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.1,\n",
    "            class_weight={-1:1.3, 1:1.2},\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        lgb_stage2.fit(X_train_trades, y_train_trades)\n",
    "        stage2_pred = lgb_stage2.predict(X_test_trades)\n",
    "        temp_preds[trade_indices] = stage2_pred\n",
    "\n",
    "    all_preds.extend(temp_preds)\n",
    "    all_true.extend(y_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Evaluate\n",
    "# -----------------------------\n",
    "print(classification_report(all_true, all_preds))\n",
    "print(confusion_matrix(all_true, all_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175cbf44-ea01-445e-9e48-ce193bbde50e",
   "metadata": {},
   "source": [
    "### Random Forest (single-stage, 3-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ae5412f-9538-4ac2-99a7-f360ba246828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.44      0.46      0.45     97752\n",
      "           0       0.51      0.81      0.62     78332\n",
      "           1       0.49      0.24      0.33    100036\n",
      "\n",
      "    accuracy                           0.48    276120\n",
      "   macro avg       0.48      0.50      0.47    276120\n",
      "weighted avg       0.48      0.48      0.45    276120\n",
      "\n",
      "[[44697 31231 21824]\n",
      " [11624 63121  3587]\n",
      " [45933 29759 24344]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Prepare data\n",
    "# -----------------------------\n",
    "X = df.drop(columns=['target', 'timestamp'])\n",
    "y = df['target']\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Time-series CV\n",
    "# -----------------------------\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Median imputation (no leakage)\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_test = imputer.transform(X_test)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3️⃣ Random Forest model\n",
    "    # -----------------------------\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=10,\n",
    "        min_samples_leaf=200,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "\n",
    "    all_preds.extend(preds)\n",
    "    all_true.extend(y_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Evaluation\n",
    "# -----------------------------\n",
    "print(classification_report(all_true, all_preds))\n",
    "print(confusion_matrix(all_true, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9438e-896c-4c47-9b74-fa72bf7eb4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c5f135b-761d-44d5-91bf-560b7925a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.30      0.01      0.02     97752\n",
      "           0       0.54      0.70      0.61     78332\n",
      "           1       0.45      0.77      0.57    100036\n",
      "\n",
      "    accuracy                           0.48    276120\n",
      "   macro avg       0.43      0.49      0.40    276120\n",
      "weighted avg       0.42      0.48      0.38    276120\n",
      "\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[  884 23848 73020]\n",
      " [ 1166 54543 22623]\n",
      " [  849 22058 77129]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ Prepare data\n",
    "# =========================\n",
    "X = df.drop(columns=['target', 'timestamp'])\n",
    "y = df['target']\n",
    "\n",
    "# Replace inf with NaN\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ Time-series CV loop\n",
    "# =========================\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "    print(f\"Fold {fold}\")\n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # -------------------------\n",
    "    # Impute using TRAIN only\n",
    "    # -------------------------\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_test  = imputer.transform(X_test)\n",
    "\n",
    "    # -------------------------\n",
    "    # Random Forest model\n",
    "    # -------------------------\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=12,\n",
    "        min_samples_leaf=150,\n",
    "        max_features='sqrt',\n",
    "        class_weight={-1:1.0, 0:0.8, 1:1.2},\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # -------------------------\n",
    "    # Probability predictions\n",
    "    # -------------------------\n",
    "    proba = rf.predict_proba(X_test)\n",
    "    class_order = rf.classes_  # e.g. [-1, 0, 1]\n",
    "\n",
    "    idx_sell = np.where(class_order == -1)[0][0]\n",
    "    idx_hold = np.where(class_order == 0)[0][0]\n",
    "    idx_buy  = np.where(class_order == 1)[0][0]\n",
    "\n",
    "    # -------------------------\n",
    "    # Threshold tuning\n",
    "    # -------------------------\n",
    "    preds = []\n",
    "    for p in proba:\n",
    "        if p[idx_buy] > 0.38:\n",
    "            preds.append(1)\n",
    "        elif p[idx_sell] > 0.38:\n",
    "            preds.append(-1)\n",
    "        else:\n",
    "            preds.append(0)\n",
    "\n",
    "    all_preds.extend(preds)\n",
    "    all_true.extend(y_test)\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ Final evaluation\n",
    "# =========================\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(all_true, all_preds))\n",
    "\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(all_true, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91a8132-862b-419d-a1fe-cbf1b71bed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
